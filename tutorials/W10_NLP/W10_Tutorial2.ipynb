{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W10_Tutorial2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CIS-522/course-content/blob/W10T1-waxing/tutorials/W10_NLP/W10_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn57geGyp2hO"
      },
      "source": [
        "# CIS-522 Week 10 Part 2\n",
        "# Introduction to Transformers, BERT, and Language Models\n",
        "\n",
        "**Instructor:** Lyle Ungar\n",
        "\n",
        "**Content Creators:** Sanjeevini  Ganni, Ameet Rahane, Byron Galbraith\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymkujLBlqM6F"
      },
      "source": [
        "##Tutorial Objectives\n",
        "\n",
        "\n",
        "(1) Recognize NLP tasks: IR/search,  Question Answering/text completion, MT \\\\\n",
        "(2) Understand distributional similarity on words and context including Context-oblivious embeddings (word2vec, glove, fastText) and multilingual embeddings  \\\\\n",
        "(3) Attention  \\\\\n",
        "(4) Context-sensitive embeddings: BERT and transformers: masking and self-attention \\\\\n",
        "(5) The many flavors of BERT:  RoBERTa and DistilBERT \\\\\n",
        "(6) Fine-tuning language embeddings \\\\\n",
        "(7) Explaining NLP models \\\\\n",
        "(8) Big language models: GPT-3 and friends  \\\\\n",
        "(9) Societal: Bias in language embeddings \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2Bqp7M12U1"
      },
      "source": [
        "---\n",
        "## Preface\n",
        "We recommend saving this notebook in your Google Drive (`File -> Save a copy in Drive`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsvDRvFG15Rw",
        "cellView": "form"
      },
      "source": [
        "#@markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
        "my_pennkey = '' #@param {type:\"string\"}\n",
        "my_pod = 'Select' #@param ['Select', 'euclidean-wombat', 'sublime-newt', 'buoyant-unicorn', 'lackadaisical-manatee','indelible-stingray','superfluous-lyrebird','discreet-reindeer','quizzical-goldfish','ubiquitous-cheetah','nonchalant-crocodile','fashionable-lemur','spiffy-eagle','electric-emu','quotidian-lion','astute-jellyfish', 'quantum-herring']\n",
        "\n",
        "# start timing\n",
        "import time\n",
        "try:t0;\n",
        "except NameError: t0 = time.time()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_oC9dNZqQu-"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RcjygJ2CRSG",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "!pip install torchtext==0.4.0\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!pip install . \n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52U7jyR4pngL",
        "cellView": "form"
      },
      "source": [
        "#@title Imports and Seed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "% matplotlib inline\n",
        "import re\n",
        "from IPython.display import Image\n",
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchtext import data, datasets\n",
        "from torchtext.vocab import Vectors, FastText\n",
        "import fasttext\n",
        "import requests\n",
        "import zipfile\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "seed = 522\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l82sJ8BNmty-",
        "cellView": "form"
      },
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline \n",
        "fig_w, fig_h = (8, 6)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "SMALL_SIZE = 12\n",
        "\n",
        "\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/\"\n",
        "              \"course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik8be94eRLPt",
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions\n",
        "def cosine_similarity(vec_a, vec_b):\n",
        "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
        "        return np.dot(vec_a, vec_b) / \\\n",
        "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
        "def tokenize(sentences):\n",
        "  #Tokenize the sentence\n",
        "  #from nltk.tokenize library use word_tokenize\n",
        "  token = word_tokenize(sentences)\n",
        "  \n",
        "  return token\n",
        "\n",
        "def plot_train_val(x, train, val, train_label, val_label, title):\n",
        "  plt.plot(x, train, label=train_label)\n",
        "  plt.plot(x, val, label=val_label)\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi_7aepGAZkf"
      },
      "source": [
        "###Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZ56QJ96aMW",
        "cellView": "form"
      },
      "source": [
        "#@title Load Data\n",
        "\n",
        "def load_dataset(emb_vectors, sentence_length = 50):\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=sentence_length)\n",
        "    LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "    TEXT.build_vocab(train_data, vectors=emb_vectors)\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    train_data, valid_data = train_data.split(split_ratio=0.7, random_state = random.seed(seed))\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "\n",
        "    return TEXT, vocab_size, train_iter, valid_iter, test_iter\n",
        "\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWxI-yHszqqs"
      },
      "source": [
        "---\n",
        "##Section 1: Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J7i4TZQAIix",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Transformers\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"b2MGGyowt0A\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJhcL3PivRyM"
      },
      "source": [
        "Transformers! Like CNNs and LSTMs, this base model architecture has been the foundation of many very successful models such as BERT and friends. It has seen tremendous use in the NLP space, and there have been effforts to extend it to the audio and visual domains as well. The original paper, [Attention Is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762), is very readable and you are encouraged to take a look.\n",
        "\n",
        "The Transformer model is fundamentally an encoder-decoder model that operates on sequences of tokens. Both the encoder and decoder components are composed of stacks of submodules that use **only** attention mechanisms and linear weights to learn (there are no CNNs or RNNs). The architecture schematic looks like the following:\n",
        "\n",
        "![transformer architecture](https://d2l.ai/_images/transformer.svg)\n",
        "\n",
        "In the rest of this section we will be going over the various building blocks that go into Transformers. The goal here is not to train anything (that is left for a homework assignment). Rather, the emphasis is on understanding what all the pieces do and how they fit together.\n",
        "\n",
        "*Note:* Many of the images in this section are taken from Dive Into Deep Learning's chapter on [Attention Mechanisms](https://d2l.ai/chapter_attention-mechanisms/index.html). You are encouraged to check that out for additional details and implementations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjdBlVNuT96x"
      },
      "source": [
        "### Self-Attention\n",
        "\n",
        "Transformers make use of something called self-attention as a critical component to the entire operation. What does that mean in the context of attention mechanisms? If you recall, attention mechanisms in machine learning have three components:\n",
        "\n",
        " - the values V (the things you perceive i.e. model inputs)\n",
        " - the query Q (the thing you want to attend to)\n",
        " - the keys K (a mapping between queries and values)\n",
        " \n",
        "Generally the number and dimensionality of queries and values can all be different. In self-attention, the queries, keys, and values are all drawn from the same set of inputs. In other words, we don't need to specify anything about what and how queries and keys are formed, as they come straight from the data just like the values!\n",
        "\n",
        "Ok, so we know that our queries, keys, and values come from our input sequence, but which attention mechanism should we use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djMSZnkXkeDU"
      },
      "source": [
        "### Masked Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvk5zc2akcM0",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Masking\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"VtaGIp_9j1w\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLCjYZvPR4-A"
      },
      "source": [
        "Masking is used to hide values from being attended to, including (a) words that are hidden for self-supervised learning, (b) padding tokens, and (c) in the seq2seq tasks in the original Transformers paper, everything that came after the next token in the output training (to enforce autoregressive behavior). \n",
        "\n",
        "BERT, on the other hand, masks out individual tokens within the input sequence and everything but those tokens on the output side, which provides a more bidirectional-style embedding learning strategy. This is accomplished by setting every element we want to mask to $-\\infty$ before applying softmax, which has the effect of giving that element a probability of 0.\n",
        "\n",
        "We've provided a masked softmax function below, which assumes a binary matrix of size (batch_size, n_tokens) where a value of 0 indicates that token should be masked.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kodsdE5dedu"
      },
      "source": [
        "def masked_softmax(x, mask):\n",
        "  \"\"\" Applies softmax on a masked version of the input.\n",
        "  Args:\n",
        "    x (n_batch, n_tokens, t_tokens): - the scaled dot product of Q and K\n",
        "    mask (n_batch, n_tokens): - binary mask, all values = 0 will be set to -inf\n",
        "  Returns:\n",
        "    (n_batch, n_tokens, n_tokens): the result of applying softmax along the last\n",
        "                                   dimension of the masked input.\n",
        "  \"\"\"\n",
        "  return F.softmax(x.masked_fill_(mask.unsqueeze(1) == 0, float('-inf')), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKRJULI3CM_b"
      },
      "source": [
        "#### Exercise 1: Masked Scaled Dot Product Attention Module\n",
        "In this exercise you will implement the forward method of a PyTorch module for computing the above masked scaled dot product attention function. PyTorch provides us with the very useful [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) function to compute matrix multiplication over batches while preserving the batch dimension.\n",
        "\n",
        "*NOTE:* Dropout is normally applied to the `scaled_dot_product` quantity before softmax is applied during training. However, in the interests of clarity, we are omitting it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KguOtPV5ClSA"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, embed_dim):    \n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  def forward(self, queries, keys, values, mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
        "      keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
        "      values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
        "      mask (n_batch, n_tokens, embed_dim): mask (M) tensor of either 1's or -inf's\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): scaled dot product attention tensor\n",
        "    \"\"\"\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"ScaledDotProductAttention\")\n",
        "    ####################################################################\n",
        "    scaled_dot_product = ...    \n",
        "    masked_softmax_scores = ...\n",
        "    attention = ...\n",
        "    return attention\n",
        "\n",
        "# Uncomment below to test your module\n",
        "# torch.manual_seed(522)\n",
        "# batch_size, n_tokens, embed_dim = 1, 3, 4\n",
        "# tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "# attention = ScaledDotProductAttention(embed_dim)\n",
        "# mask = torch.ones((batch_size, n_tokens))\n",
        "# print(attention(tokens, tokens, tokens, mask))\n",
        "# mask[0, 2:] = 0\n",
        "# print(attention(tokens, tokens, tokens, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04wkZTV8FrlI"
      },
      "source": [
        "# to_remove solution\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  def forward(self, queries, keys, values, mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
        "      keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
        "      values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
        "      mask (n_batch, n_tokens): binary mask tensor\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): scaled dot product attention tensor\n",
        "    \"\"\"\n",
        "    scaled_dot_product = torch.bmm(queries, torch.transpose(keys, 1, 2)) / np.sqrt(self.embed_dim)\n",
        "    masked_softmax_scores = masked_softmax(scaled_dot_product, mask)\n",
        "    attention = torch.bmm(masked_softmax_scores, values)\n",
        "    return attention\n",
        "\n",
        "torch.manual_seed(522)\n",
        "batch_size, n_tokens, embed_dim = 1, 3, 4\n",
        "tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "attention = ScaledDotProductAttention(embed_dim)\n",
        "mask = torch.ones((batch_size, n_tokens))\n",
        "print(attention(tokens, tokens, tokens, mask))\n",
        "mask[0, 2:] = 0\n",
        "print(attention(tokens, tokens, tokens, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2fVCkImZNxq"
      },
      "source": [
        "Self-attention is great, but it has two shortcomings:\n",
        "\n",
        "1. It doesn't let us specify or control what gets attended to and thus will converge on only one strategy due to averaging.\n",
        "\n",
        "2. There is no implicit ordering or notion of relative position of the input tokens to each other unlike in RNNs and ConvNets.\n",
        "\n",
        "We know things about natural language, such as that word order matters and there are many different grammatical and syntactic features that imbue useful meaning. How do we overcome this?\n",
        "\n",
        "First, let's address the attention strategy problem. One answer to only having a single attention strategy is to have many!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onssr49pLFiK"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "Multi-head attention is a mechanism employed by Transformers to concurrently learn multiple different attention strategies or \"heads.\" This is accomplished by passing each of the queries, keys, and values through single, fully-connected linear layers. Attention training is then conducted on all splits, which then get joined together at the end and passed through a linear layer to achieve the final output.\n",
        "\n",
        "![multi-head attention diagram](https://d2l.ai/_images/multi-head-attention.svg)\n",
        "\n",
        "Now, to avoid poor scaling performance with each attention head we add, we can take advantage of the fact that we only need to compute matrix multiplications. By effectively making the dimensionality of the newly made query, key, and value heads equal to the original embedding dimension cleanly divided by the number of heads, we can keep the heads strided in one tensor and thus compute the attention scores of all heads in a single call.\n",
        "\n",
        "The methods to shuffle the data around for the input values (queries, keys, values) and then to unshuffle it for the output are provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdhsAo_3qa6"
      },
      "source": [
        "def mha_transform_input(x, n_heads, head_dim):\n",
        "  \"\"\" Restructure the input tensors to compute the heads in parallel\n",
        "  Requires that head_dim = embed_dim / n_heads\n",
        "  Args:\n",
        "    x (n_batch, n_tokens, embed_dim): input tensor, one of queries, keys, or values\n",
        "    n_heads (int): the number of attention heads\n",
        "    head_dim (int): the dimensionality of each head\n",
        "  Returns:\n",
        "    (n_batch*n_heads, n_tokens, head_dim): 3D Tensor containing all the input heads\n",
        "  \"\"\"\n",
        "  n_batch, n_tokens, _ = x.shape    \n",
        "  x = x.reshape((n_batch, n_tokens, n_heads, head_dim))\n",
        "  x = x.permute(0, 2, 1, 3)\n",
        "  return x.reshape((n_batch * n_heads, n_tokens, head_dim))\n",
        "  \n",
        "def mha_transform_output(x, n_heads, head_dim):\n",
        "  \"\"\" Restructures the output back to the original format \n",
        "  Args:\n",
        "    x (n_bacth*n_heads, n_tokens, head_dim): multi-head representation tensor\n",
        "    n_heads (int): the number of attention heads\n",
        "    head_dim (int): the dimensionality of each head\n",
        "  Returns:\n",
        "    (n_batch, n_tokens, embed_dim): 3D Tensor containing all the input heads\n",
        "  \"\"\"\n",
        "  n_concat, n_tokens, _ = x.shape\n",
        "  n_batch = n_concat // n_heads\n",
        "  x = x.reshape((n_batch, n_heads, n_tokens, head_dim))\n",
        "  x = x.permute(0, 2, 1, 3)\n",
        "  return x.reshape((n_batch, n_tokens, n_heads * head_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_rWeVEtKgdk"
      },
      "source": [
        "#### Exercise 2: Multi-Head Attention Module\n",
        "In this exercise you will implement the the forward method of a PyTorch module for handling the multi-head attention mechanism. Each of the Q, K, and V inputs need to be run through their corresponding linear layers and then transformed using `mha_transform_input`. You then pass these to our scaled dot product attention module, transform that output back using `mha_transform_output`, and then run that through the corresponding output linear layer.\n",
        "\n",
        "*NOTE:* In the original Transformers paper, the linear layers were just weight matrices with no bias term which is reproduced here by using `Linear` layers and setting bias to False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ohoi8GWGN8l"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = embed_dim // n_heads\n",
        "    \n",
        "    self.attention = ScaledDotProductAttention(embed_dim)\n",
        "    self.query_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    self.key_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    self.value_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    self.out_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "  \n",
        "  def forward(self, queries, keys, values, mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
        "      keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
        "      values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
        "      mask (n_batch, n_tokens): binary mask tensor\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): multi-head attention tensor\n",
        "    \"\"\"\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"MultiHeadAttention\")\n",
        "    ####################################################################\n",
        "    q_heads = ...\n",
        "    k_heads = ...\n",
        "    v_heads = ...\n",
        "\n",
        "    attention_heads = ...\n",
        "    attention = ...\n",
        "    return attention\n",
        "\n",
        "# Uncomment below to test your module\n",
        "# torch.manual_seed(522)\n",
        "# n_heads, batch_size, n_tokens, embed_dim = 2, 1, 3, 4\n",
        "# tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "# mask = torch.ones((batch_size, n_tokens))\n",
        "# attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "# attention(tokens, tokens, tokens, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32T1AYtlGqdX"
      },
      "source": [
        "# to_remove solution\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = embed_dim // n_heads\n",
        "    \n",
        "    self.attention = ScaledDotProductAttention(embed_dim)\n",
        "    self.query_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    self.key_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    self.value_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    self.out_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "  \n",
        "  def forward(self, queries, keys, values, mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
        "      keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
        "      values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
        "      mask (n_batch, n_tokens): binary mask tensor\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): multi-head attention tensor\n",
        "    \"\"\"\n",
        "    q_heads = mha_transform_input(self.query_fc(queries), self.n_heads, self.head_dim)\n",
        "    k_heads = mha_transform_input(self.key_fc(keys), self.n_heads, self.head_dim)\n",
        "    v_heads = mha_transform_input(self.value_fc(values), self.n_heads, self.head_dim)\n",
        "\n",
        "    attention_heads = self.attention(q_heads, k_heads, v_heads, mask)\n",
        "    attention = self.out_fc(mha_transform_output(attention_heads, self.n_heads, self.head_dim))\n",
        "    return attention\n",
        " \n",
        "torch.manual_seed(522)\n",
        "n_heads, batch_size, n_tokens, embed_dim = 2, 1, 3, 4\n",
        "tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "mask = torch.ones((batch_size, n_tokens))\n",
        "attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "attention(tokens, tokens, tokens, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVRwWnKf3ZN"
      },
      "source": [
        "So we have a solution for enabling multiple different attention strategies to be applied. But how about knowing where each token is in the sequence? Well, what if we just explicitly added some piece of information to the input representation that encodes each token's position?\n",
        "\n",
        "While you can likely imagine ways to do this, Transformers use the following scheme:\n",
        "\n",
        "$$\n",
        "p_{i,2j} = \\sin \\left( \\frac{i}{10000^{2j/d}} \\right) \\\\\n",
        "p_{i,2j+1} = \\cos \\left( \\frac{i}{10000^{2j/d}} \\right)\n",
        "$$\n",
        "\n",
        "where $i$ and $j$ are iterated over the rows (tokens) and columns (embedding dimensions), respectively. This likely seems strange at first, but it has the neat effect of\n",
        "1. providing unique values across the matrix elements\n",
        "2. using float values which easily add to the input embedded tokens\n",
        "\n",
        "We can see an example of what this looks like when plotted for a few columns below:\n",
        "\n",
        "![positional embedding](https://d2l.ai/_images/output_self-attention-and-positional-encoding_d76d5a_31_0.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpM1VQmqLXQT"
      },
      "source": [
        "### Exercise 3: Positional Encoding Module\n",
        "In this exercise you will create the forward method for a PyTorch module that will add positional embeddings to an input batch of tokens. The position embedding values are already computed and cached for you.\n",
        "\n",
        "*NOTE:* Dropout is normally applied to the output of this module during training, but we have omitted it for clarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8cAATbDAiA4",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Positional Encoding\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"FoRWkEAJDtg\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elotwfaIIM1o"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, max_len=1000):\n",
        "    super().__init__()\n",
        "    self.position_embedding = torch.zeros((1, max_len, embed_dim))\n",
        "    i = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)\n",
        "    j2 = torch.arange(0, n_hidden, step=2, dtype=torch.float32)\n",
        "    x = i / torch.pow(10000, j2 / n_hidden)\n",
        "    self.position_embedding[..., 0::2] = torch.sin(x)\n",
        "    self.position_embedding[..., 1::2] = torch.cos(x)        \n",
        "\n",
        "  def forward(self, x):\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"PositionalEncoder\")\n",
        "    ####################################################################\n",
        "    x_plus_p = ...\n",
        "    return x_plus_p\n",
        "\n",
        "# Uncomment below to test your module\n",
        "# n_tokens, embed_dim = 40, 40\n",
        "# pos_enc = PositionalEncoder(embed_dim) \n",
        "# p = pos_enc(torch.zeros((1, n_tokens, embed_dim)))\n",
        "# plt.imshow(p.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYPyST5chpIH"
      },
      "source": [
        "# to_remove solution\n",
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, max_len=1000):\n",
        "    super().__init__()\n",
        "    self.position_embedding = torch.zeros((1, max_len, embed_dim))\n",
        "    i = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)\n",
        "    j2 = torch.arange(0, embed_dim, step=2, dtype=torch.float32)\n",
        "    x = i / torch.pow(10000, j2 / embed_dim)\n",
        "    self.position_embedding[..., 0::2] = torch.sin(x)\n",
        "    self.position_embedding[..., 1::2] = torch.cos(x)        \n",
        "\n",
        "  def forward(self, x):\n",
        "    x_plus_p = x + self.position_embedding[:, :x.shape[1]]\n",
        "    return x_plus_p\n",
        "  \n",
        "with plt.xkcd():\n",
        "  n_tokens, embed_dim = 40, 40\n",
        "  pos_enc = PositionalEncoder(embed_dim) \n",
        "  p = pos_enc(torch.zeros((1, n_tokens, embed_dim)))\n",
        "  plt.imshow(p.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N7_j4uihthC"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "We now have almost everything we need to assemble the full Transformer network. There are just two more modules we need to quickly discuss, and then we will get to putting them all together.\n",
        "\n",
        "First, there is the residual layer norm that appears after many of the other components. In all cases, this takes the output of the previous component, sums it with the input to that component (the residual connection), and then normalizes the result across the layer.\n",
        "\n",
        "Second is a two layer fully connected module with a ReLU activation in between. This appears after the attention components.\n",
        "\n",
        "These are provided below. Note that dropout would normally be applied in various places in these modules during training, but we have omitted it for clarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAF3g4h51_dA"
      },
      "source": [
        "class ResidualNorm(nn.Module):\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "  \n",
        "  def forward(self, x, residual):\n",
        "    return self.norm(x + residual)\n",
        "\n",
        "class Feedforward(nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(embed_dim, hidden_dim)    \n",
        "    self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.fc2(F.relu(self.fc1(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DSh0XVD1_Nl"
      },
      "source": [
        "Now that we have all the modules we need, we can begin assembling the bigger modules. First we will look at the Encoder Block. The actual encoder will be made up of some number of these stacked together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkOaVXrnU-43"
      },
      "source": [
        "#### Exercise 4: Encoder Block Module\n",
        "In this exercise you will create the forward method of the PyTorch module representing the Encoder Block of the Transformer. The Encoder Block has the following architecture:\n",
        "1. a multi-head attention module using self-attention\n",
        "2. 1st residual layer norm\n",
        "3. feed-forward model\n",
        "4. 2nd residual layer norm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwIxWxc3S7CT"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim, hidden_dim):\n",
        "    super().__init__()    \n",
        "    self.attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "    self.norm1 = ResidualNorm(embed_dim)\n",
        "    self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
        "    self.norm2 = ResidualNorm(embed_dim)\n",
        "\n",
        "  def forward(self, src_tokens, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src_tokens (n_batch, n_tokens, embed_dim): the source sequence\n",
        "      src_mask (n_batch, n_tokens): binary mask over the source\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): the encoder state\n",
        "    \"\"\"\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"EncoderBlock\")\n",
        "    ####################################################################\n",
        "    self_attention = ...\n",
        "    normed_attention = ...\n",
        "    ff_out = ...\n",
        "    out = ...\n",
        "    return out\n",
        "\n",
        "# Uncomment below to test your module\n",
        "# torch.manual_seed(522)\n",
        "# n_heads, batch_size, n_tokens, embed_dim, hidden_dim = 2, 1, 3, 4, 8\n",
        "# tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "# mask = torch.ones((batch_size, n_tokens))\n",
        "# encoder = EncoderBlock(n_heads, embed_dim, hidden_dim)\n",
        "# encoder(tokens, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I572BgTdinzD"
      },
      "source": [
        "# to_remove solution\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim, hidden_dim):\n",
        "    super().__init__()    \n",
        "    self.attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "    self.norm1 = ResidualNorm(embed_dim)\n",
        "    self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
        "    self.norm2 = ResidualNorm(embed_dim)\n",
        "\n",
        "  def forward(self, src_tokens, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src_tokens (n_batch, n_tokens, embed_dim): the source sequence\n",
        "      src_mask (n_batch, n_tokens): binary mask over the source\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): the encoder state\n",
        "    \"\"\"\n",
        "    self_attention = self.attention(src_tokens, src_tokens, src_tokens, src_mask)\n",
        "    normed_attention = self.norm1(self_attention, src_tokens)\n",
        "    ff_out = self.feedforward(normed_attention)\n",
        "    out = self.norm2(ff_out, normed_attention)\n",
        "    return out\n",
        "\n",
        "torch.manual_seed(522)\n",
        "n_heads, batch_size, n_tokens, embed_dim, hidden_dim = 2, 1, 3, 4, 8\n",
        "tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "mask = torch.ones((batch_size, n_tokens))\n",
        "encoder = EncoderBlock(n_heads, embed_dim, hidden_dim)\n",
        "encoder(tokens, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgRqX5_c8nbm"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "Like the encoder, the decoder is made up of a stack of repeating Decoder Blocks. Decoder Blocks are similar to the Encoder ones with an additional multi-head attention component that doesn't use self-attention, but instead gets the queries from the decoder's self-attention component and the keys and values from the encoder's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GqPoroY9WEv"
      },
      "source": [
        "#### Exercise 5: Decoder Block Module\n",
        "In this exercise you will create the forward method of the PyTorch module representing the Decoder Block of the Transformer. The Decoder Block has the following architecture:\n",
        "1. a multi-head attention using self-attention\n",
        "2. 1st residual layer norm\n",
        "3. a 2nd multi-head attention that incorporates the encoder output\n",
        "4. 2nd residual layer norm\n",
        "5. feed-forward model\n",
        "6. 3rd residual layer norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2C7z-h4Awqt"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim, hidden_dim):\n",
        "    super().__init__()    \n",
        "    self.self_attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "    self.norm1 = ResidualNorm(embed_dim)\n",
        "    self.encoder_attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "    self.norm2 = ResidualNorm(embed_dim)\n",
        "    self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
        "    self.norm3 = ResidualNorm(embed_dim)\n",
        "\n",
        "  def forward(self, tgt_tokens, tgt_mask, encoder_state, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      tgt_tokens (n_batch, n_tokens, embed_dim): the target sequence\n",
        "      tgt_mask (n_batch, n_tokens): binary mask over the target tokens\n",
        "      encoder_state (n_batch, n_tokens, embed_dim): the output of the encoder pass\n",
        "      src_mask (n_batch, n_tokens): binary mask over the source tokens\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): the decoder state\n",
        "    \"\"\"\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"DecoderBlock\")\n",
        "    ####################################################################\n",
        "    self_attention = ...\n",
        "    normed_self_attention = ...\n",
        "    encoder_attention = ...\n",
        "    normed_encoder_attention = ...\n",
        "    ff_out = ...\n",
        "    out = ...\n",
        "    return out\n",
        "\n",
        "# Uncomment below to test your module\n",
        "# torch.manual_seed(522)\n",
        "# n_heads, batch_size, n_tokens, embed_dim, hidden_dim = 2, 1, 3, 4, 8\n",
        "# tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "# src_mask = torch.ones((batch_size, n_tokens))\n",
        "# tgt_mask = torch.ones((batch_size, n_tokens))\n",
        "# encoder = EncoderBlock(n_heads, embed_dim, hidden_dim)\n",
        "# decoder = DecoderBlock(n_heads, embed_dim, hidden_dim)\n",
        "# encoder_state = encoder(tokens, src_mask)\n",
        "# decoder(tokens, tgt_mask, encoder_state, src_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Uin8O4AzA9"
      },
      "source": [
        "# to_remove solution\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim, hidden_dim):\n",
        "    super().__init__()    \n",
        "    self.self_attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "    self.norm1 = ResidualNorm(embed_dim)\n",
        "    self.encoder_attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "    self.norm2 = ResidualNorm(embed_dim)\n",
        "    self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
        "    self.norm3 = ResidualNorm(embed_dim)\n",
        "\n",
        "  def forward(self, tgt_tokens, tgt_mask, encoder_state, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      tgt_tokens (n_batch, n_tokens, embed_dim): the target sequence\n",
        "      tgt_mask (n_batch, n_tokens): binary mask over the target tokens\n",
        "      encoder_state (n_batch, n_tokens, embed_dim): the output of the encoder pass\n",
        "      src_mask (n_batch, n_tokens): binary mask over the source tokens\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): the decoder state\n",
        "    \"\"\"\n",
        "    self_attention = self.self_attention(tgt_tokens, tgt_tokens, tgt_tokens, tgt_mask)\n",
        "    normed_self_attention = self.norm1(self_attention, tgt_tokens)\n",
        "    encoder_attention = self.encoder_attention(normed_self_attention, encoder_state, encoder_state, src_mask)\n",
        "    normed_encoder_attention = self.norm2(encoder_attention, normed_self_attention)\n",
        "    ff_out = self.feedforward(normed_encoder_attention)\n",
        "    out = self.norm2(ff_out, normed_encoder_attention)\n",
        "    return out\n",
        "\n",
        "torch.manual_seed(522)\n",
        "n_heads, batch_size, n_tokens, embed_dim, hidden_dim = 2, 1, 3, 4, 8\n",
        "tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
        "src_mask = torch.ones((batch_size, n_tokens))\n",
        "tgt_mask = torch.ones((batch_size, n_tokens))\n",
        "encoder = EncoderBlock(n_heads, embed_dim, hidden_dim)\n",
        "decoder = DecoderBlock(n_heads, embed_dim, hidden_dim)\n",
        "encoder_state = encoder(tokens, src_mask)\n",
        "decoder(tokens, tgt_mask, encoder_state, src_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz0tL-xaBLkv"
      },
      "source": [
        "### Summary\n",
        "We've covered all the building blocks that make up the Transformer network architecture. The module versions presented here were often simplified in some ways and made more verbose in others to emphasize what each component is doing. We also haven't demonstrated the assembly of the stacked encoder, stacked decoder, full joint model, and training here, and instead leave that as a homework exercise for you to explore. You can also see the version that comes baked into PyTorch in the [nn.Transformer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html) module code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "SblV7fWsC4jx"
      },
      "source": [
        "#@title Video : Transformer Architecture\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"_sKZpAptIZk\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMptHSvoaIz"
      },
      "source": [
        "*Estimated time: 85 minutes since start*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G31Vd2hVzuQt"
      },
      "source": [
        "---\n",
        "##Section 2: BERT and friends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4b-wW01AUQ8",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Bert and Friends\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"gEkmPb0140w\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwJPWkxHj8a8"
      },
      "source": [
        "---\n",
        "## Section 3: BERT\n",
        "\n",
        "BERT, or Bidrectional Encoder Representations from Transforms, is a Transformer-based machine learning technique for NLP pre-training developed by Google. The original English BERT has two models: \n",
        "\n",
        "1. BERT$_{BASE}$: $12$ encoders with $12$ bidirectional self-attention heads\n",
        "2. BERT$_{LARGE}$: $24$ encoders with $24$ bidirectional self-attention heads \n",
        "\n",
        "Both models are pre-trained with unlabeled data extracted from BooksCorpus with $800$M words and Wikipedia with $2500$M words. Importantly, unlike context-free models like GloVe or word2vec, BERT takes context into account for each occurrence of a given word.  For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkGd01-Wsthf",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Using BERT\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"sFQGoswoaeI\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRmL_Thxoi46"
      },
      "source": [
        "*Estimated time: 95 minutes since start*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlRZjEpPtMAm"
      },
      "source": [
        "---\n",
        "## Section 4: RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ2X4VP0ssNE"
      },
      "source": [
        "As can be seen by the name, RoBERTa builds on BERT, modifying key hyperparameters. It removes the next-sentence pretraining objective and trains with much larger mini-batches and learning rates. \n",
        "\n",
        "Spend some time playing with RoBERTa natural language inference at https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+I+lost+an+animal.+. Additionally, spend some time looking at the other model examples at https://github.com/huggingface/transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPuXB-HxTv1s"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import RobertaConfig, RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base', is_decoder = True, add_cross_attention = True)\n",
        "\n",
        "test_input = tokenizer('Hello, myh dog is cute', return_tensors = 'pt')\n",
        "outputs = model(**test_input)\n",
        "outputs.last_hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9abr3heRKxc"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install datasets wandb transformers\n",
        "import datasets\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8l4uNCtOZYG"
      },
      "source": [
        "Let's download the IMDB data that we looked at in week 9! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEWJs8MjOQog"
      },
      "source": [
        "train_data, test_data = datasets.load_dataset('imdb', split =['train', 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtejroLcSZtq"
      },
      "source": [
        "We want to initialize RoBERTa from the pretrained model, which we can do easily with huggingface. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23DoRTT7SYF-"
      },
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBBwcH8yqbE6"
      },
      "source": [
        "Before we can get to training, we need to first write a tokenizer to deal with our input data and we need to create a few accuracy metrics. Notably, we'll be looking at accuracy, f1, precision, and recall. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gWyElMjShX1"
      },
      "source": [
        "def tokenization(batched_text):\n",
        "    return tokenizer(batched_text['text'], padding = True, truncation=True)\n",
        "\n",
        "train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\n",
        "test_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))\n",
        "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LoIvuF5S-hL"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CepqmNxNvOb5"
      },
      "source": [
        "We use the `TrainingArguments` function to set the hyperparameters of our training funtion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ9j5YcwS_gQ"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = '.',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size = 16,\n",
        "    gradient_accumulation_steps = 16,    \n",
        "    per_device_eval_batch_size= 8,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    disable_tqdm = False, \n",
        "    load_best_model_at_end=True,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps = 8,\n",
        "    fp16 = True,\n",
        "    logging_dir='./logging/',\n",
        "    dataloader_num_workers = 8,\n",
        "    run_name = 'roberta-classification'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qB89Ve9vXbO"
      },
      "source": [
        "We initialize TensorBoard - which is what we'll be using to view our results and train using the `Trainer` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWArc7tKT212"
      },
      "source": [
        "%load_ext tensorboard\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data\n",
        ")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVU8L70BT60R"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx8aeifJUG_F"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJrKIzalXyQY"
      },
      "source": [
        "%tensorboard --logdir logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP1rPKAOooDd"
      },
      "source": [
        "*Estimated time: 110 minutes since start*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEgqsbk5zy0J"
      },
      "source": [
        "---\n",
        "## Section 5: BERT variations (DistilBERT)\n",
        "\n",
        "https://arxiv.org/abs/1910.01108\n",
        "\n",
        "\n",
        "DistilBERT, as the name suggests, is a \"distilled\" version of BERT: smaller, faster, cheaper, and lighter. Often times, having very large models is infeasible, as it requires a lot of compute time and resources. Specifically, we often need to run models on smaller devices, without the ability to run many large GPUs to train. DistilBERT is a pre-trained general-purpose language representation model, which we can then fine-tune to achieve good performance on a number of tasks. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vJP1ZEaDXbO"
      },
      "source": [
        "Let's use DistilBERT to write a small question answering system. Question answering systems automatically respond to a given query. The input will be framed with context and the question. For example:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Context :\n",
        "The US has passed the peak on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month. The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world. \\\\\n",
        "Question:\n",
        "What was President Donald Trump's prediction?\n",
        "\n",
        "\n",
        "---\n",
        "Answer: \n",
        "some states would reopen this month.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA4C_ZpPD7PD"
      },
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',return_token_type_ids = True)\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad', return_dict=False)\n",
        "\n",
        "context = \"The US has passed the peak on new coronavirus cases, \" \\\n",
        "          \"President Donald Trump said and predicted that some states would reopen this month. \" \\\n",
        "          \"The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\"\n",
        "\n",
        "question = \"What was President Donald Trump's prediction?\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(question, context)\n",
        "\n",
        "\n",
        "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
        "\n",
        "ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]\n",
        "answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)\n",
        "\n",
        "print (\"\\nQuestion: \",question)\n",
        "print (\"\\nAnswer Tokens: \")\n",
        "print (answer_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "print (\"\\nAnswer : \",answer_tokens_to_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkLAUmkbEyeK"
      },
      "source": [
        "Cool! Go ahead and try your own questions and see how DistilBERT answers it! Let's try multiple questions at once (in a batch). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxjk7g7iE8lT"
      },
      "source": [
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',return_token_type_ids = True)\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad', return_dict = False)\n",
        "\n",
        "context = \"The US has passed the peak on new coronavirus cases, \" \\\n",
        "          \"President Donald Trump said and predicted that some states would reopen this month.\" \\\n",
        "          \"The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, \" \\\n",
        "          \"the highest for any country in the world.\"\n",
        "\n",
        "print (\"\\n\\nContext : \",context)\n",
        "\n",
        "questions = [\"What was President Donald Trump's prediction?\",\n",
        "             \"How many deaths have been reported from the virus?\",\n",
        "             \"How many cases have been reported in the United States?\"]\n",
        "\n",
        "question_context_for_batch = []\n",
        "\n",
        "for question in questions :\n",
        "    question_context_for_batch.append((question, context))\n",
        "\n",
        "encoding = tokenizer.batch_encode_plus(question_context_for_batch,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "for index,(start_score,end_score,input_id) in enumerate(zip(start_scores,end_scores,input_ids)):\n",
        "    max_startscore = torch.argmax(start_score)\n",
        "    max_endscore = torch.argmax(end_score)\n",
        "    ans_tokens = input_ids[index][max_startscore: max_endscore + 1]\n",
        "    answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens, skip_special_tokens=True)\n",
        "    answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "    print (\"\\nQuestion: \",questions[index])\n",
        "    print (\"Answer: \", answer_tokens_to_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoyEwsPPozXz"
      },
      "source": [
        "*Estimated time: 120 minutes since start*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUmSToDB3bFH"
      },
      "source": [
        "---\n",
        "## Section 6: Explaining language models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2EiG7Rs3kvb",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Explaining language models\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"G38ZZNnXaQs\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aKo3ReY_9sT"
      },
      "source": [
        "#### Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mkr7X0a__ko",
        "cellView": "form"
      },
      "source": [
        "#@markdown Why would you expect part of speech tagging to be done closer to the input, and co-reference to be done more deeply in the network?\n",
        "#report to Airtable\n",
        "NLP_network_structure = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzJOIertAVq9",
        "cellView": "form"
      },
      "source": [
        "#@markdown Why are byte pair encodings problematic for using \"feature importance\" to understand what words \"cause\" a model to make a given prediction?\n",
        "BPE_interpretation = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9cTkXEsAuV4",
        "cellView": "form"
      },
      "source": [
        "#@markdown Attention turns out not to be a great way of finding the most important words used by a model. Why not? (Hint: where might attention focus on the sentence: \"The movie was long and boring.\"?)\n",
        "interpreting_attention = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdrEjN3n31Wx"
      },
      "source": [
        "There are lots of tools out there to help visualize what's going on in NLP systems. If you want (this is not an assignment), play around with the demos at https://pair-code.github.io/lit/demos/.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9lghzopo3LF"
      },
      "source": [
        "*Estimated time: 130 minutes since start*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52dVCGNMz9w-"
      },
      "source": [
        "---\n",
        "## Section 7: Bias in Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AU4_g3Ddoli",
        "cellView": "form"
      },
      "source": [
        "#@title Video : Bias in Embeddings\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"meUnCri_52c\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfmYlSc_0BQz"
      },
      "source": [
        "You just saw how training on large amounts of historical text can introduce undesirable associations and outcomes. In this section we are going to explore this idea further as it pertains to coreference resolution.\n",
        "\n",
        "Coreference resolution is the NLP task of finding all the terms that refer to an entity in a passage of text e.g. what noun phrase does a pronoun refer to. This can be quite difficult, even for humans, if the passage is ambiguous enough.\n",
        "\n",
        "For example, in the sentence:\n",
        "\n",
        "`The customer asked to speak with the manager because he wanted to fix the billing error quickly.`\n",
        "\n",
        "what does `he` refer to? We can reasonably assume given the context that `he` refers to the customer. Furthermore, it shouldn't matter which pronoun (he/she/they) was in that spot, it should still refer back to the customer.\n",
        "\n",
        "However this is not the case with some models! For example, here is the output of Huggingface's [Neural Coreference model](https://github.com/huggingface/neuralcoref) when we use `she` as the pronoun:\n",
        "\n",
        "![coref with she](https://imgur.com/4FmN3ZC.png)\n",
        "\n",
        "You can see that `she` is scored against all detected noun phrases and gets the highest score with `the customer`. So far so good. Now let's try it with `he` instead:\n",
        "\n",
        "![coref with he](https://imgur.com/SOyLrtg.png)\n",
        "\n",
        "The model has instead associated `he` with `the manager`, and quite strongly at that, even though that doesn't make sense contextually. As this is a neural-based model trained on historical data, one possibility is there were many instances where `he` and `manager` were associated, enough to get \"distracted\" by that signal over the rest of the sentence.\n",
        "\n",
        "\n",
        "\n",
        "As was mentioned in the video, many people are actively working toward both identifying and mitigating these undesirable behaviors and also educating researchers, practitioners, and the general public about these issues. For instance, the sample sentence used above was taken from the [Winogender Schemas](https://github.com/rudinger/winogender-schemas), a set of sample sentences to check the variance in outcomes when only a single pronoun is changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftnc8GGUtL7O"
      },
      "source": [
        "#### Exercise 6: Explore Bias in Coreference Resolution Models\n",
        "\n",
        "Two different coreference resolution models that have nice online demos are from [Huggingface](https://huggingface.co/coref/) and [AllenNLP](https://demo.allennlp.org/coreference-resolution). In this exercise, you will explore a variety of sentences with these two tools and see how they compare. Try the following sentences in both and see how they handle the change in pronoun:\n",
        "\n",
        "`The doctor berated the nurse. He had come in late for the meeting.`\n",
        "\n",
        "`The doctor berated the nurse. She had come in late for the meeting.`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClZxGxhJBZa0"
      },
      "source": [
        "#### Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUAhscXBBdMX",
        "cellView": "form"
      },
      "source": [
        "#@markdown Did Huggingface get it right?\n",
        "huggingface_bias = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAsS-ChpBuQv",
        "cellView": "form"
      },
      "source": [
        "#@markdown Did Allen Institute get it right?\n",
        "#report to Airtable\n",
        "allenInst_bias = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKW9WhInB5GM",
        "cellView": "form"
      },
      "source": [
        "#@markdown How might you \"fine tune\" Bert to reduce such errors?\n",
        "#report to Airtable\n",
        "fine_tune_away_bias = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbgTqgB5rH_W"
      },
      "source": [
        "---\n",
        "# Wrap up\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98YXpSntrY6k",
        "cellView": "form"
      },
      "source": [
        "#@markdown #Run Cell to Show Airtable Form\n",
        "#@markdown ##**Confirm your answers and then click \"Submit\"**\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import urllib.parse\n",
        "from IPython.display import IFrame\n",
        "def prefill_form(src, fields: dict):\n",
        "  '''\n",
        "  src: the original src url to embed the form\n",
        "  fields: a dictionary of field:value pairs,\n",
        "  e.g. {\"pennkey\": my_pennkey, \"location\": my_location}\n",
        "  '''\n",
        "  prefill_fields = {}\n",
        "  for key in fields:\n",
        "      new_key = 'prefill_' + key\n",
        "      prefill_fields[new_key] = fields[key]\n",
        "  prefills = urllib.parse.urlencode(prefill_fields)\n",
        "  src = src + prefills\n",
        "  return src\n",
        "\n",
        "\n",
        "#autofill time if it is not present\n",
        "try: t0;\n",
        "except NameError: t0 = time.time()\n",
        "try: t1;\n",
        "except NameError: t1 = time.time()\n",
        "try: t2;\n",
        "except NameError: t2 = time.time()\n",
        "try: t3;\n",
        "except NameError: t3 = time.time()\n",
        "try: t4;\n",
        "except NameError: t4 = time.time()\n",
        "try: t5;\n",
        "except NameError: t5 = time.time()\n",
        "try: t6;\n",
        "except NameError: t6 = time.time()\n",
        "try: t7;\n",
        "except NameError: t7 = time.time()\n",
        "\n",
        "# autofill fields if they are not present\n",
        "# a missing pennkey and pod will result in an Airtable warning\n",
        "# which is easily fixed user-side.\n",
        "try: my_pennkey;\n",
        "except NameError: my_pennkey = \"\"\n",
        "try: my_pod;\n",
        "except NameError: my_pod = \"Select\"\n",
        "try: NLP_network_structure;\n",
        "except NameError: NLP_network_structure = \"\" \n",
        "try: BPE_interpretation;\n",
        "except NameError: BPE_interpretation = \"\" \n",
        "try: interpreting_attention;\n",
        "except NameError: interpreting_attention = \"\" \n",
        "try: huggingface_bias;\n",
        "except NameError: huggingface_bias = \"\" \n",
        "try: allenInst_bias;\n",
        "except NameError: allenInst_bias = \"\" \n",
        "try: fine_tune_away_bias;\n",
        "except NameError: fine_tune_away_bias = \"\" \n",
        "\n",
        "times = np.array([t1,t2,t3,t4,t5,t6,t7])-t0\n",
        "\n",
        "fields = {\n",
        "    \"my_pennkey\": my_pennkey,\n",
        "    \"my_pod\": my_pod, \n",
        "    \"NLP_network_structure\": NLP_network_structure,\n",
        "    \"BPE_interpretation\": BPE_interpretation,\n",
        "    \"interpreting_attention\": interpreting_attention,\n",
        "    \"huggingface_bias\": huggingface_bias,\n",
        "    \"allenInst_bias\": allenInst_bias,\n",
        "    \"fine_tune_away_bias\": fine_tune_away_bias,\n",
        "    \"cumulative_times\": times\n",
        "}\n",
        "\n",
        "src = \"https://airtable.com/embed/shrfeQ4zBWMSZSheB?\"\n",
        "\n",
        "display(IFrame(src = prefill_form(src, fields), width = 800, height = 400))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDE0MJbb5dLH"
      },
      "source": [
        "## Feedback\n",
        "How could this session have been better? How happy are you in your group? How do you feel right now?\n",
        "\n",
        "Feel free to use the embeded form below or use this link:\n",
        "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://airtable.com/shrNSJ5ECXhNhsYss\">https://airtable.com/shrNSJ5ECXhNhsYss</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPPjyA-H5kLE"
      },
      "source": [
        "display(IFrame(src=\"https://airtable.com/embed/shrNSJ5ECXhNhsYss?backgroundColor=red\", width = 800, height = 400))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}