{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week10Homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CIS-522/course-content/blob/W10T1-waxing/tutorials/W10_NLP/W10_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i79mWlGhdH2T"
      },
      "source": [
        "#CIS Week 10 Homework\n",
        "\n",
        "**Instructor:** Lyle Ungar\n",
        "\n",
        "**Content Creators:** Brittany Shields, Alessandra Rossi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27G1cTCwbFj4",
        "cellView": "form"
      },
      "source": [
        "#@markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
        "my_pennkey = 'bfranklin' #@param {type:\"string\"}\n",
        "my_pod = 'euclidean-wombat' #@param ['Select', 'euclidean-wombat', 'sublime-newt', 'buoyant-unicorn', 'lackadaisical-manatee','indelible-stingray','superfluous-lyrebird','discreet-reindeer','quizzical-goldfish','ubiquitous-cheetah','nonchalant-crocodile','fashionable-lemur','spiffy-eagle','electric-emu','quotidian-lion','astute-jellyfish', 'quantum-herring']\n",
        "\n",
        "# start timing\n",
        "import time\n",
        "try:t0;\n",
        "except NameError: t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dF811j6dfb8"
      },
      "source": [
        "We **strongly** recommend that you keep a separate document offline with your answers, and paste them in when you're ready to submit. Colab may reset and clear your notebook after a period of inactivity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuuPO_pvdhcB"
      },
      "source": [
        "###**Learning Objectives**\n",
        "Understand the Transfomer model end-to-end.\n",
        "\n",
        "How might word embedding perpetuate systemic bias?\n",
        "\n",
        "How might we work with policymakers and companies to reduce the spread of fake news?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t7WSNmVuUDb"
      },
      "source": [
        "## Part I: The Full Transformer Model\n",
        "\n",
        "In the tutorial workbook you implemented versions of all the key building blocks needed to assemble the full Transformer model. For this homework, you will actually do that and wire everything up in order to have a complete, end-to-end version of a Transformer network that you could then train (though the goal is to just get it to successfully run a prediction step). \n",
        "\n",
        "If you haven't already, you should read the original paper [Attention Is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762) to get a good sense of the full architecture and motivations. You can copy all the previous modules over from the tutorial and, if so inclined, add dropout back in to all the places it has been removed from. While the approach taken in this course is somewhat different from that in [Dive Into Deep Learning's version](https://d2l.ai/chapter_attention-mechanisms/transformer.html), you can use their code as a fairly close reference. The Transformer module provided with PyTorch ([nn.Transformer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html)) may also prove a useful reference, though it assumes all embedding/positional encoding/token prediction happens outside the module itself.\n",
        "\n",
        "To recap, you already created the following components:\n",
        "* Masked scaled dot product attention\n",
        "* Multi-head attention\n",
        "* Positional encoder\n",
        "* Residual norm\n",
        "* Position-wise feedforward network\n",
        "* Encoder block\n",
        "* Decoder block\n",
        "\n",
        "There are three main modules left to build: the encoder stack, the decoder stack, and the full transformer model that combines these two.\n",
        "\n",
        "**Encoder Stack**\n",
        "\n",
        "This module will take as paramaters every parameter you need for the Encoder block as well as the number of blocks to chain in the stack and the vocabulary size (for creating the embedding layer). The forward method of the module takes as input the source text sequence and mask. The source sequence will be passed through an embedding layer, then the positional encoding layer, and finally, along with the mask, through all the Encoder blocks.\n",
        "\n",
        "References: [D2L](https://d2l.ai/chapter_attention-mechanisms/transformer.html#encoder), [PyTorch](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoder)\n",
        "\n",
        "**Decoder Stack**\n",
        "\n",
        "This module will take as paramaters every parameter you need for the Decoder block as well as the number of blocks to chain in the stack and the vocabulary size (for creating the embedding layer). The forward method of the module takes as input the target text sequence, target mask, encoder output, and source sequence mask. The target sequence will be passed through an embedding layer, then the positional encoding layer, and finally, along with the target mask, encoder state, and source mask, through all the Decoder blocks.\n",
        "\n",
        "References: [D2L](https://d2l.ai/chapter_attention-mechanisms/transformer.html#decoder), [PyTorch](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoder)\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "This module will take in all the parameters you need for the encoder and decoders. In addition to creating both the encoder and decoder in the module init method, you will also create a final linear layer which projects your decoder output back to vocab size for token prediction. The forward method of the module will take in the source sequence and the target sequence, compute the corresponding masks, run the source through the encoder, run the target and encoder output through the decoder, then pass the decoder output through the final linear layer to make the next token prediction.\n",
        "\n",
        "References: [D2L](https://d2l.ai/chapter_attention-mechanisms/transformer.html#training), [PyTorch](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ7iz5bsd0uF"
      },
      "source": [
        "## Part II: Primer on Gender Bias in Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQzJNYPseA_c"
      },
      "source": [
        "Read the following primer that describes the societal implications of gender bias in word embeddings: \\\\\n",
        "“Man is to Doctor as Woman is to Nurse: The Gender Bias of Word Embeddings: Why we should worry about gender inequality in Natural Language Processing techniques” \\\\\n",
        "https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17 \\\\\n",
        "*9 min read*, Tommaso Buonocore, 2019\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2yJzN7CeS3m",
        "cellView": "form"
      },
      "source": [
        "#@markdown Question 1: Identify a compelling quote or example from the article that resonated with you.  How does it emphasize the societal responsibility of data scientists?\n",
        "q1 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "try:t1;\n",
        "except NameError: t1 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3j4iFbnepdX"
      },
      "source": [
        "##Part III: Tech Paper on Gender Bias in Word Embeddings\n",
        "\n",
        "In the previous article, the author referenced that data scientists are introducing new strategies to reduce biases in word embeddings.  In 2016, researchers from Boston University and Microsoft Research published a paper with significant findings demonstrating the prevalence of gender bias in word embeddings, as well as proposed mitigation measures.  A 5-minute summary of the technical components of the paper can be found [here](https://towardsdatascience.com/tackling-gender-bias-in-word-embeddings-c965f4076a10) and the original paper can be found [here](https://arxiv.org/abs/1607.06520). \\\n",
        "\n",
        "“Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings” \\\n",
        "https://arxiv.org/abs/1607.06520 \\\n",
        "*30 min read*, 2016\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZXjlLvAdjk6",
        "cellView": "form"
      },
      "source": [
        "#@markdown Question 2: In the “Discussion” section, the authors consider the responsibility of machine language programmers with regards to bias in word embeddings.  Describe their stance.\n",
        "q2 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown Question 3: How could word embeddings amplify societal stereotypes?\n",
        "q3 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown Question 4: How could the authors’ proposed technological interventions address this concern?\n",
        "q4 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "try:t2;\n",
        "except NameError: t2 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex7vA7rfgpFq"
      },
      "source": [
        "##Part IV: Fake News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9TVVjSogsvL"
      },
      "source": [
        "The following article provides an overview of neural fake news, including potential defenses against it as well as suggestions for further research.\n",
        "\n",
        "“An Exhaustive Guide to Detecting and Fighting Neural Fake News using NLP” \\\n",
        "•\thttps://www.analyticsvidhya.com/blog/2019/12/detect-fight-neural-fake-news-nlp/ \\\n",
        "•\t*16-minute read*, Analytics Vidhya, 2019\n",
        "\n",
        "In 2019, Jack Clark, the Policy Director at OpenAI, gave testimony to the US House Intelligence Committee.  You can either read his written 10-page testimony here or watch his live testimony here (jump to minute 11:00 to watch his 5 minute testimony).\n",
        "\n",
        "Written Testimony of Jack Clark, Policy Director, OpenAI \\\n",
        "•\thttps://intelligence.house.gov/uploadedfiles/clark_deepfakes_sfr.pdf \\\n",
        "•\tor \\\n",
        "•\thttps://www.c-span.org/video/?461679-1/house-intelligence-committee-hearing-deepfake-videos \\\n",
        "*Jump to minute 11:00* \\\n",
        "•\tHouse Permanent Select Committee on Intelligence, 2019 \\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmh3fCcChEyw",
        "cellView": "form"
      },
      "source": [
        "#@markdown Question 5: Jack Clark enumerated several ideas for interventions.  Discuss one of the specific ideas he suggested.\n",
        "q5 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown Question 6: How might technologists work with policymakers and companies to prevent the spread of fake news?\n",
        "q6 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown Question 7: •\tOpenAI is not making GPT-3 publicly available, claiming that they are concerned that it might be used for doing bad. Do you think that was a good decision? Why?\n",
        "q7 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "try:t3;\n",
        "except NameError: t3 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5md7iimhYEo"
      },
      "source": [
        "## PART V: Know your Pod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl_r2SsehjCI"
      },
      "source": [
        "With two other members of your pod, do the following. Have each of them recommend you their favorite song. Then, listen to those songs and write down your thoughts for each. (~100 words each)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je_h2CB-hdX8",
        "cellView": "form"
      },
      "source": [
        "know_a_pod_1 = \"\" #@param{type:\"string\"}\n",
        "know_a_pod_2 = \"\" #@param{type:\"string\"}\n",
        "\n",
        "try:t4;\n",
        "except NameError: t4 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}