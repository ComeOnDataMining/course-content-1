### Question
Besides gradient clipping, can you think of any other methods to cope with gradient explosion in recurrent neural networks?

### Answer
Using specialized RNNs having "gates" that are better at retaining long term dependencies.
